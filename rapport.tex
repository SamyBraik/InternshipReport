\documentclass[a4paper,12pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{float}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{comment}
\usepackage{subcaption}


\usepackage[backend=biber,style=numeric,citestyle=numeric]{biblatex}
\addbibresource{references.bib}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Page setup
\geometry{margin=1in}

% Title
\title{Synthetic turbulent velocity field using statistics}
\author{Samy Braik}
\date{\today}

\begin{document}

\maketitle

%\begin{abstract}
% Brief summary of your report.
%\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
Turbulence is the term used to describe the unpredictable and complex behavior of a fluid's motion. It is characterized by vortex. It is opposed to a laminar flow, where the particles' movement is pre. Formally, the behavior of a fluid is quantified by the Reynolds number (Re), the higher this number is the more turbulence the fluid is. 

\bigskip
In this work, we study an ideal type of turbulence flow called Homogeneous and Isotropic Turbulence (HIT). \\
There are few properties that characterized this type of flow. It is homogeneous which means that the statistical properties (mean, variance, correlation functions) do not depend on position in space, and it is isotropic which means that these statistics are invariant under rotations or reflections of the coordinate.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{illustrations/TurbulenceExample.jpg}
    \caption{Slice through the field of the scalar dissipation reveals the small-scale structure of turbulence from CNRS UMR 6614 CORIA and JSC}
\end{figure}

%First, the spatial average should be 0 for each component. Since the flow is isotropic, the turbulence properties should remain invariant to translation or axis rotations.

\bigskip

When the goal is to use statistical methods to produce turbulent velocity field two paths are worth considering. The first one is to simply use a statistical method that generate the velocity field directly. This is done in \cite{Yousif_Yu_Lim_2022},\cite{wang2025fourierflowfrequencyawareflowmatching} or \cite{parikh2025conditionalflowmatchinggenerative}. \\
Here, generative models seem the most suitable. Two major difficulties could be encountered, the huge cost and the poor generalization of the method. Indeed, in order to train such a model, there is a need for highly resolved turbulence usually produced with Direct Numerical Simulation (DNS) which are costly numerical methods. Apart from that, there is the natural cost of the method itself that can't be sidelined. The poor generalization of the method stem from the fact that usually, the models are trained on fields with a specific Reynolds number which leads to poor robustness if the turbulence parameters are changed. \\
The other path would be to start from an already existent method. For example in the case of Reynolds-averaged Navier–Stokes equations (RANS) technique (see \cite{} for details), we can use statistical methods to learn a closure model, like it is done in \cite{Bezgin2021}. This allows a better theoretical guarantee on the produced field and also get rid of the robustness problem from the previous case.  

%\newpage

\section{Model}

\subsection{Random Fourier}

We set ourselves in the random Fourier model developed in \cite{Janin2021} and briefly remind it. 
Some papers that study turbulence start from a velocity field generated by DNS and use Fourier transform to study the field properties in the spectral space. The motivation behind this model, is to perform an inverse Fourier transform to generate a synthetic turbulent field with a good choice of Fourier coefficient. \\
This leads to the following expression of the velocity field 

\begin{align}
    u^s(x,t)=\int_{-\infty}^\infty\int_{-\infty}^{\infty}\left[ \hat{u}(\kappa,\omega)e^{l\psi(\kappa,\omega)}\sigma(\kappa,\omega) \right] e^{l(\kappa\cdot x + \omega t)} d\kappa d\omega
\end{align}
Discretizing the expression in N random Fourier modes and taking into account that the field is real, it could be written 
\begin{align}
    u^s(x,t)= 2 \sum_{i=1}^N \hat{u}_n \cos(\kappa^n\cdot x + \psi_n + \omega_n t)\sigma^n
\end{align}
Furthermore, we place ourselves in the following work in a froze turbulence which means that we ignore the effect of time. Therefore, the expression if simplified and we have
\begin{align}
    u^s(x,t)= 2 \sum_{n=1}^N \hat{u}_n \cos(\kappa^n\cdot x + \psi_n)\sigma^n
    \label{RandomFourierModel}
\end{align}
For each $n^{th}$ Fourier mode associated with the wave vector $\kappa^n$, $\hat{u}_n$ is the amplitude, $\psi_n$ the phase, $\sigma^n$ the direction and $\omega_n$ the time-frequency. \\
Like previously mentioned, a good choice of the Fourier coefficient is needed in order to produce a realistic velocity field. \cite{Janin2021} features a discussion on the choice of $\hat{u}, \kappa, \psi_n$ and $\sigma$.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{illustrations/Velocity_Example.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{illustrations/Velocity_Example2.png}
    \end{minipage}
    \caption{Synthetic velocity obtain using the Random Fourier model}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{illustrations/VelocityFieldVTU.png}
    \caption{Synthetic velocity obtain using the Random Fourier model represented by arrows}
\end{figure}

\subsection{Coefficient choice} \label{Coefficient choice}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{illustrations/WaveVectorGeometry.png}
    \caption{Wave vector geometry of the $n^\text{th}$ Fourier mode from \cite{Janin2021}}
    \label{WaveVectorGeometry}
\end{figure}
To ensure the statistical isotropy of the generated field, the wave vector $\kappa$ is chosen randomly on the half-sphere.
This leads to the following components choices
\begin{align}
    \kappa_1 &= \sin(\theta)\cos(\varphi) \label{kappa1}\\ 
    \kappa_2 &= \sin(\theta)\sin(\varphi) \label{kappa2}\\
    \kappa_2 & = \cos(\theta) \label{kappa3}
\end{align}
with the angles chosen randomly according to the following probability density functions $f_\theta(x)=\frac{sin(x)}{2}$ and $f_\varphi(x)=\frac{1}{2\pi}\mathbbm{1}_{[0,2\pi]}$. \\
Regarding the choice of $\sigma$, it is such that $\kappa\cdot\sigma=0$ is verified, which is implied by the divergence-free condition $\nabla\cdot u^s=0$. \\
Therefore, 

\begin{align}
    \sigma_1&=\cos(\varphi)\cos(\theta)\cos(\alpha)-\sin(\varphi)\sin(\alpha) \label{sigma1}\\
    \sigma_2&=\sin(\varphi)\cos(\theta)\cos(\alpha)+\cos(\theta)\sin(\alpha) \label{sigma2}\\
    \sigma_3&=-\sin(\theta)\cos(\alpha) \label{sigma3}
\end{align}

with $\alpha$ chose according to a uniform distribution on $[0,2\pi]$ i.e. $f_\alpha(x)=\frac{1}{2\pi}\mathbbm{1}_{[0,2\pi]}$. \\
The phase coefficient $\psi$ is randomly chosen according to $\mathcal{U}[0,2\pi]$ to ensure spatial homogeneity. \\
Lastly, the amplitude $\hat{u}_n=\sqrt{E(\kappa_n)\delta \kappa_n}$ where $\delta \kappa_n = \frac{\log(\kappa_N)-\log(\kappa_1)}{N}$ and $\kappa_n=e^{(\log(\kappa_1)+n\delta \kappa_n)}$. \\
The method to compute the prescribed energy spectrum $E(\kappa_n)$ is described in the next subsection.

\subsection{Energy spectrum}
%In a turbulent setup, an energy cascade is observed where a transfer of energy from large eddies to small scales (or sometimes the opposite). \\
Large eddies that carry energy distribute it through the energy cascade to smaller scales until it's dissipated by viscous effect at Kolmogorov scales. 
This energy transfer across different scales is characterized by the energy spectrum, noted $E(\kappa)$. It describes how much kinetic energy is attached to a given wave number $\kappa$. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{illustrations/energy-spectrum-example.png}
    \caption{Energy spectrum of a turbulent flow from \cite{phdthesisRies}}
\end{figure}

In the initial range, energy is injected into the flow, for instance, by an obstacle, rotating machinery, or another mechanism. The flow is being dominated with big whirlpools. \\
The inertial range is where the energy is transferred. The large eddies are broken down into smaller eddies and so on.  Finally, in the dissipative range, the viscous effects dominate and break down the turbulence.

\bigskip

In the context of homogeneous isotropic turbulence, Kolmogorov gave the following formula  

\begin{align}
    E(\kappa) = C_k \kappa^{-5/3}\varepsilon^{2/3}
\end{align}
where $\varepsilon$ is the turbulent dissipation, $\kappa$ the wave number and $C_k$ the Kolmogorov constant.

Although this formula captures the behavior in the inertial range and introduces this famous $-\frac{5}{3}$ slop, it fails at reconstructing the spectrum in the initial and dissipative range i.e. at low and high wave numbers. To obtain a more complete recovery of the energy spectrum, \cite{Janin2021} uses, among others, the von-Kármán Pao (VKP) energy spectrum.
\bigskip

It is defined by 

\begin{align}
    E_{\text{VKP}}(\kappa)=\frac{2}{3}\alpha_e k L_e \frac{(kL_e)^4}{[(kL_e)^2+1]^{17/6}}\exp(-2(\kappa L_\eta)^2)
\end{align}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{illustrations/Energy_Spectrum_VKP.png}
    \caption{VKP energy spectrum vs theoretical spectrum}
    \label{VKPspectrum}
\end{figure}

In order to accurately reconstruct the theoretical spectrum, a sufficient number of computational Fourier modes is required.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{illustrations/EnergySpectrumTrained.png}
    \caption{VKP energy spectrum vs theoretical spectrum with}
\end{figure}

\begin{comment}
The real energy can be then computed 

\begin{align}
    E = \int_{0}^{\infty}E_{\text{VKP}}(\kappa) d\kappa
\end{align}
\end{comment}

\subsection{Limitations and goals}
The assessment of the synthetic velocity field's quality could be done by looking at few metrics. First, the reconstructed energy spectrum should match as close as possible the theoretical spectrum. Then, for each component, the average should be 0 and the Root Mean Square (RMS) values should match the one used to build the turbulence in the first place. Lastly, we look at the shape of the velocity increments. \\

\begin{definition}[RMS]
    Let $x=(x_1,\ldots x_n)$ a real vector. The RMS is defined by 
    \begin{align}
        x_\text{RMS} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2}
    \end{align}
\end{definition}

\begin{definition}[Velocity increments]
    Let $u$ be a velocity field. For $r>0$ and a point $x$, the velocity increments are defined by 
    \begin{align}
        \delta_r u = u(x+r)-u(x) 
        \label{DefVelIncr}
    \end{align}
\end{definition}

The model gives us the following results for each metric 

\begin{table}[h]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Direction} & \textbf{Value} \\
\midrule
x & -0.0026918877847492695 \\
y & -0.0001003775978460908 \\
z & -0.0012576839653775096 \\
\bottomrule
\end{tabular}
\caption{Velocity mean}
\label{BaseMean}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Direction} & \textbf{Value (expected : 0.222)} \\
\midrule
x & 0.19583044946193695 \\
y & 0.17598901689052582 \\
z & 0.19306901097297668 \\
\bottomrule
\end{tabular}
\caption{RMS speed}
\label{BaseRMSspeed}
\end{minipage}
\end{table}


\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{illustrations/TransVelIncrExample.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{illustrations/LongVelIncrExample.png}
    \end{minipage}
    \caption{Velocity increments in blue with standard Gaussian in red dots}
\end{figure}


\bigskip
Although, the model retrieve a correct energy spectrum (Figure \ref{VKPspectrum}) and limited anisotropy, the velocity increments obtained are Gaussian. It is known that velocity increments appear to be non-Gaussian and more particularly heavy-tailed distribution. For instance \cite{} shows that velocity increments display heavy taildness when $r$ is small in \ref{DefVelIncr}. It is due ...

Furthermore, the following figures show that the more turbulent a flow is, the heavier the tails of its velocity increments are.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{illustrations/TransVelIncrRe.png}
        \caption{Variation of the transverse velocity derivative PDF with the Reynolds number from \cite{GotohVelIncr}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{illustrations/LongVelIncrRe.png}
        \caption{Variation of the longitudinal velocity derivative PDF with the Reynolds number from \cite{GotohVelIncr}}
    \end{minipage}
\end{figure}

\bigskip
With the random Fourier model in mind, the goal is to identify which coefficient could lead to more realistic velocity increments without disrupting the other good properties that are already satisfied. 


\section{Method}
To tackle the problem, no obvious path stood out. 
The one chosen is simply to define a model and a loss and optimize the model's parameters. \\
Since the goal is to retrieve heavy-tailed velocity increments, the logical criteria to look out is the kurtosis of those increments. 


\begin{definition}[Kurtosis]
    Let $X$ be a real random variable, $\mu$ its mean and $\sigma$ its standard deviation. Its kurtosis is defined by 
    \begin{align}
    \text{kurt}(X)=\mathbb{E}\left[\left(\frac{X-\mu}{\sigma}\right) \right]^2    
    \end{align}
\end{definition}

\begin{remark}
    For a Gaussian distribution, which is the distribution of the velocity increments in the first place, the kurtosis is equal to 3.
\end{remark}

\begin{remark}
    In practice, we look at the flatness which is simply the $\text{kurt}-3$.
\end{remark}
Therefore, in order to push for heavy tailed velocity increments, we came up with a straight forward flatness loss. 

\begin{definition}[Flatness loss]
    Let $F_\text{target}$ be the target flatness, $x=(x_1,\ldots,x_n)$ be the input vector.
    \begin{align}
        \mathcal{L}_F (x) = \frac{1}{N}\sum_{i=1}^{N}\left( \text{kurt}-3 - F_\text{target}  \right) ^2 
    \end{align}
\end{definition}

To compute the energy spectrum as close as the theoretical spectrum, we simply look at the MSE with an added smoothness term. This gave us the following loss

\begin{definition}[Energy spectrum loss]
    Let $\alpha>0$, $E_\text{rec}$ the reconstructed spectrum and $E\text{theory}$ the theoretical spectrum.
    We define a smoothness term by 
    \begin{align}
        S(E_\text{rec}) = \frac{1}{N-2}\sum_{i=2}^{N-1}\left(E_{\text{rec},i-1} -2E_{\text{rec},i} + E_{\text{rec},i+1}   \right)
    \end{align}
    The loss is then defined by 
    \begin{align}
        \mathcal{L}_{ES} = \frac{1}{N} \sum_{i=1}^{N} (E_{\text{rec},i}-E_{\text{theory},i})^2 + \alpha S(E_\text{rec}) 
    \end{align} 
\end{definition}



\bigskip
Most of the learning and training has been done simply on the parameter by defining them as torch Parameter. It is fairly simple model (probably the simplest) but led some great results. 

[Mieux décrire le model (nn.Parameter)]

\subsection{Preliminary work}
In order to check the robustness of the random Fourier model in the first place, we conducted a simple experiment. We set the target flatness as 3, meaning we look for Gaussian velocity increments. Since it is the base model velocity increments, we want to know if we will retrieve the parameterization introduce in \cite{Janin2021} or will it move away. We look up two situations : starting from Gaussian velocity increments and heavy-tailed velocity increments.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{illustrations/StartGaussian.png}
    \caption{Angles distributions starting from Gaussian velocity increments}
\end{figure}

Here, the angles distributions stay sensibly the same, with a slight change in $\alpha$. This is expected since we already satisfy the target flatness.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{illustrations/StartHeavyTail.png}
    \caption{Angles distributions starting from heavy-tailed velocity increments}
\end{figure}

In this case, $\varphi$ and $\alpha$ stay relatively stable but $\theta$ is drawn to a more spread distribution. 

\bigskip
In both cases Gaussian velocity increments are indeed recovered, the RMS speed are in the same range as in \ref{BaseRMSspeed} and the generated velocity field does average to 0.

\subsection{Phase coefficient}
To improve the situation, our first idea was to work on the phase parameter $\psi$ in \ref{RandomFourierModel}, with the idea of finding a better distribution than $\mathcal{U}([0,2\pi])$. A simple experiment consisting of changing $\mathcal{U}[0,2\pi]$ to arbitrary distribution like $\mathcal{N}(-10,130), \mathcal{E}(1)$ or $\mathcal{\ldots}$ showed that it has no effect on the shape of the velocity increments distribution. Indeed, the reason is pretty straight forward : the Central Limit Theorem (CLT). \\
Sampling I.I.D. values of $\psi$ according to any of these distributions and plugging it in $\ref{RandomFourierModel}$ with $N$ big enough (which is expected in our situation) lead directly to an application of the CLT. 

\bigskip


\subsection{Phase, wave vector and direction}
The natural follow-up was to look at all the coefficient at the same time, without changing the objectives nor the method. It led to heavy tailed velocity increments indeed as shown in figure \ref{}

This approach led to promising results as the volicity increments obtained displayed heavy tailed.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VelIncrPSIKAPPASIGMA/VelIncrLong.png}
        \label{fig:VelIncrLong3}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VelIncrPSIKAPPASIGMA/VelIncrTrans.png}
        \label{fig:VelIncrTrans3}
    \end{minipage}
    \caption{Velocity increments with learned coefficient \((\psi,\kappa,\sigma)\)}
\end{figure}

The reconstructed spectrum \ldots

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{VelIncrPSIKAPPASIGMA/EnergySpectrum.png}
    \label{fig:EnergySpectrum3}
    \caption{Marginal density of the learned parameters $(\psi,\kappa,\sigma)$}
\end{figure}

On the learned parameters, we fit a Gaussian kernel density estimator and obtain the marginal distribution

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{VelIncrPSIKAPPASIGMA/Distributions.png}
    \label{fig:Distributions3}
    \caption{Marginal density of the learned parameters $(\psi,\kappa,\sigma)$}
    \label{MarginalsPsiKappaSigma}
\end{figure}

What we found is to generate new triplet $(\psi,\kappa,\sigma)$ sampling directly according to the marginal distributions from \ref{MarginalsPsiKappaSigma} doesn't work but sampling according to the obtained joint distribution led to some flatness of the velocity increments.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VelIncrPSIKAPPASIGMA/GenIncrLong.png}
        \label{fig:GenIncrLong3}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{VelIncrPSIKAPPASIGMA/GenIncrTrans.png}
        \label{fig:GenIncrTrans3}
    \end{minipage}
    \caption{Velocity increments with sampled coefficient \((\psi,\kappa,\sigma)\)}
\end{figure}

Although, this approach works as it produces heavy tailed velocity increments, there is no guarantee that the obtained $\kappa$ and $\sigma$ are orthogonal to each other as stated in \ref{Coefficient choice}. \\
Therefore, we shift our focus on the angles $(\varphi,\theta,\alpha)$ composing the $\kappa$ and $\sigma$ component. We aim at learning the three angles while using \ref{kappa1}, \ref{kappa2},\ref{kappa3} and \ref{sigma1},\ref{sigma2},\ref{sigma3} to preserve the identity $\kappa\cdot\sigma=0$.


\subsection{Angles}
Since we want to work on the angles, we need to make sure the angles are always coherent with the geometry (see \ref{WaveVectorGeometry}). Therefore, to make sure that $\varphi, \theta$ and $\alpha$ lives in their respective domain $[0,\pi], [0,\pi]$ and $[0,2\pi]$.

We train the model on 500 modes with $100^3$ points, with the turbulence parameter of \ref{TurbulenceParameters}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{illustrations/EnergySpectrumTrained.png}
    \caption{Energy spectrum reconstructed after training}
\end{figure}

and we obtain the following marginal distribution.
\begin{figure}[H]
    \centering 
    \includegraphics[width=1.0\linewidth]{illustrations/AnglesDistributionLearned.png}
    \caption{Marginal distribution of the learned angles}
\end{figure}

\section{Results and Discussion}
The work was highly exploratory and featured trials and errors along the way.

\section{Conclusion}
% Conclusion and future work.



\newpage
\appendix

\section{Turbulence parameters}
To generate the turbulence few physical and spectral parameters have to be given as input for the model. Throughout the report the parameters that have been used are listed in the following table. 

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Unit}\\
\midrule
Length of the periodic box   & $\tfrac{\pi}{6}$ & \si{\meter}\\
Number of modes              & 250 or 500 & --\\
RMS speed                    & 0.222 & \si{\meter\per\second}\\
Integral length scale        & 0.024 & \si{\meter} \\
Kinetic energy               & $1.5 \times (0.222 \times 2)$ & \si{\meter\squared\per\second\squared} \\
Viscosity                    & $1.8 \times 10^{-5}$ & \si{\meter\squared\per\second} \\
Minimum wave number          & $\tfrac{2\pi}{1.0}$ & \si{\per\meter}\\
Maximum wave number          & $\tfrac{2\pi}{0.01}$ & \si{\per\meter}\\
\bottomrule
\end{tabular}
\label{TurbulenceParameters}
\end{center}

\section{Variable selection}
\section{Variable selection}
To address the question of which angles are the most influential and which one could be negligible, we simply perform a case analysis. In the same training context, we varied individual angles while keeping others froze and untrained. It gave us the following results.
Losses obtain for different parameter combination

\begin{table}[!htb]
    \caption{Losses obtain for different parameter combination}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{250 modes}
        \begin{tabular}{ c | c | c }
        Parameters & Loss & RMS (x, y, z)\\
        \hline
        $(0,0,\alpha)$ & 123.24456 & (0.24, 0.08, 0.12)\\
        $(0, \theta, 0)$ & 1.50244 & (0.22, 0.11, 0.08)\\
        $(0, \theta,\alpha)$ & 65.67394 & (0.11, 0.22, 0.09)\\
        $(\varphi, 0, 0)$ & 0.75362 & (0.09, 0.24, 0.06)\\
        $(\varphi, 0, \alpha)$ & 0.74764 & (0.20, 0.10, 0.09)\\
        $(\varphi,\theta, 0)$ & 0.75258 & (0.12, 0.22, 0.04)\\
        $(\varphi,\theta,\alpha)$ & 2.15906 & (0.22, 0.11, 0.09)\\
    \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{500 modes}
        \begin{tabular}{ c | c | c }
        Parameters & Loss & RMS (x, y, z)\\
        \hline
        $(0,0,\alpha)$ & 0.73864 & (0.14, 0.20, 0.06)\\
        $(0, \theta, 0)$ & 0.74741 & (0.20, 0.11, 0.10)\\
        $(0, \theta,\alpha)$ & 0.79737 & (0.14, 0.21, 0.09)\\
        $(\varphi, 0, 0)$ & 0.75514 & (0.10, 0.22, 0.08)\\
        $(\varphi, 0, \alpha)$ & 0.74716 & (0.21, 0.11, 0.09)\\
        $(\varphi,\theta, 0)$ & 0.74118 & (0.22, 0.09, 0.11)\\
        $(\varphi,\theta,\alpha)$ & 0.75355 & (0.20, 0.13, 0.09)\\
        \end{tabular}
    \end{subtable} 
\end{table}


Overall, the more modes the better it is. A lot of the cases are pretty close together and lead to sensibly the same losses and in practice the desired flatness and a good recovery of the energy spectrum. \\
RMS values are affected after the learning step, depending on which angles are considered. For the moment, adding a loss on those RMS values tend to lead to less flat tails on the output velocities increments. A trade-off between RMS and flat tails seems to arise. For instance, in the case where all the parameters are learned, like in the previous section, the RMS for the z direction is off (0.09 with an expected value of 0.222).

%It appears that considering all three parameters gives the best result, which makes sense since it allow more degrees of freedom. It also highlight the importance of \(\alpha\). If we constaint the model to two parameters (or even one), including alpha consistently leads to better results. Moreover, \(\theta\) is second on the list. Constraining the number of parameters to two, the case \((\theta,\alpha)\) is preferred to the case \((\varphi,\alpha)\).

\subsection{Marginal distributions}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/alpha.png}
    \label{fig:Distributions4.1}
    \caption{Marginal density of the learned parameters \((\alpha)\)}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/theta.png}
    \label{fig:Distributions4.2}
    \caption{Marginal density of the learned parameters \((\theta)\)}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/theta_alpha.png}
    \label{fig:Distributions4.3}
    \caption{Marginal density of the learned parameters \((\theta,\alpha)\)}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/phi.png}
    \label{fig:Distributions4.4}
    \caption{Marginal density of the learned parameters \((\varphi)\)}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/phi_alpha.png}
    \label{fig:Distributions4.5}
    \caption{Marginal density of the learned parameters \((\varphi,\alpha)\)}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/phi_theta.png}
    \label{fig:Distributions4.6}
    \caption{Marginal density of the learned parameters \((\varphi,\theta)\)}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelection/phi_theta_alpha.png}
    \label{fig:Distributions4.7}
    \caption{Marginal density of the learned parameters \((\varphi,\theta,\alpha)\)}
\end{figure}

We can see that among all the cases, the final marginal distributions seem pretty consistent throughout.

\section{Variable selection with \(\psi\)}
We repeat the previous experiment considering also the phase coefficient \(\psi\). 

\bigskip
%When introducing \(\psi\), the earlier conclusion that \(\alpha\) is the most important coefficient appear to no longer hold. The three cases with the lowest loss are, in fact, those where \(\alpha\) is excluded. 

%Same observation as before could be made, the greater the loss with 250 modes, the smaller it is with 500 modes.


\begin{table}[!htb]
    \caption{Losses obtain for different parameter combination}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{250 modes}
        \begin{tabular}{ c | c | c}
        Parameters & Loss & RMS (x, y, z)\\
        \hline
        $(0,0,\alpha,\psi)$ & 0.75522 & (0.20, 0.11, 0.10)\\
        $(\varphi,0, \alpha,\psi)$ & 0.74388 & (0.09, 0.22, 0.04)\\
        $(\varphi, 0,0,\psi)$ & 0.73930 & (0.11, 0.22, 0.08)\\
        $(0, \theta, 0,\psi)$ & 24.56362 & (0.12, 0.22, 0.06)\\
        $(0, 0, 0,\psi)$ & 0.74839 & (0.09, 0.24, 0.04)\\
        $(0,\theta, \alpha,\psi)$ & 0.75160 & (0.21, 0.12, 0.10)\\
        $(\varphi,\theta,0,\psi)$ & 0.81957 & (0.09, 0.21, 0.08)\\
        $(\varphi,\theta,\alpha,\psi)$ & 183.87051 & (0.11, 0.27, 0.05)\\
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{500 modes}
        \begin{tabular}{ c | c | c}
        Parameters & Loss & RMS (x, y, z)\\
        \hline
        $(0,0,\alpha,\psi)$ & 0.74805 & (0.11, 0.25, 0.07)\\
        $(\varphi,0, \alpha,\psi)$ & 0.74443 & (0.21, 0.17, 0.06)\\
        $(\varphi, 0,0,\psi)$ & 0.74356 & (0.09, 0.27, 0.06)\\
        $(0, \theta, 0,\psi)$ & 0.74534 & (0.20, 0.11, 0.10)\\
        $(0, 0, 0,\psi)$ & 0.75040 & (0.08, 0.27, 0.06)\\
        $(0,\theta, \alpha,\psi)$ & 0.74584 & (0.20, 0.14, 0.10)\\
        $(\varphi,\theta,0,\psi)$ & 0.73757 & (0.20, 0.12, 0.11)\\
        $(\varphi,\theta,\alpha,\psi)$ &  3.38473  & (0.23, 0.11, 0.09)\\
        \end{tabular}
    \end{subtable}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/alpha_psi.png}
    \caption{Marginal density of the learned parameters \((\alpha,\psi)\)}
    \label{fig:Distributions5.1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/phi_alpha_psi.png}
    \caption{Marginal density of the learned parameters \((\varphi,\alpha,\psi)\)}
    \label{fig:Distributions5.2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/phi_psi.png}
    \caption{Marginal density of the learned parameters \((\varphi,\psi)\)}
    \label{fig:Distributions5.3}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/theta_psi.png}
    \caption{Marginal density of the learned parameters \((\theta,\psi)\)}
    \label{fig:Distributions5.4}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/psi.png}
    \caption{Marginal density of the learned parameters \((\psi)\)}
    \label{fig:Distributions5.5}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/theta_alpha_psi.png}
    \caption{Marginal density of the learned parameters \((\theta,\alpha,\psi)\)}
    \label{fig:Distributions5.6}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=0.35\textwidth]{VariableSelectionPsi/phi_theta_psi.png}
    \caption{Marginal density of the learned parameters \((\varphi,\theta,\psi)\)}
    \label{fig:Distributions5.7}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{VariableSelectionPsi/phi_theta_alpha_psi.png}
    \caption{Marginal density of the learned parameters \((\varphi,\theta,\alpha,\psi)\)}
    \label{fig:Distributions5.8}
\end{figure}
Overall, the distribution of the angles appears consistent with the previous case where \(\psi\) was not considered. The distribution of \(\psi\) itself remains pretty consistent across all cases.\\
The distribution of $\alpha$ is the most similar to the base distribution $\mathcal{U}([0,2\pi])$.\\
However, these figures represent only the marginal densities and do not capture the dependencies between parameters, which could explain the observation made from \ref{Table2}.

\subsection{Joint distributions}

\begin{figure}[htbp]
    \centering

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VariableSelectionPsi/joint_alpha_psi.png}
        \caption{Learned parameters \((\alpha, \psi)\)}
        \label{fig:joint_alpha_psi}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VariableSelectionPsi/joint_phi_psi.png}
        \caption{Joint distribution of \((\varphi, \psi)\)}
        \label{fig:joint_phi_psi}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VariableSelectionPsi/joint_theta_psi.png}
        \caption{Joint distribution of \((\theta, \psi)\)}
        \label{fig:joint_theta_psi}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VariableSelectionPsi/joint3d_phi_alpha_psi.png}
        \caption{Joint distribution of \((\varphi, \theta, \alpha)\)}
        \label{fig:joint3D_phi_alpha_psi}
    \end{subfigure}

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VariableSelectionPsi/joint3d_phi_theta_psi.png}
        \caption{Joint distribution of \((\theta, \psi)\)}
        \label{fig:joint3D_phi_theta_psi}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{VariableSelectionPsi/joint3d_theta_alpha_psi.png}
        \caption{Joint distribution of \((\varphi, \theta, \alpha)\)}
        \label{fig:joint3D_theta_alpha_psi}
    \end{subfigure}

    \caption{Joint distributions of parameter combination}
    \label{fig:joint_distributions_psi}

\end{figure}
\newpage
\printbibliography

\end{document}